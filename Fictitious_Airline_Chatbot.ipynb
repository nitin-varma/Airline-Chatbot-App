{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import re\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from transformers import pipeline\n",
        "from langchain.llms import LlamaCpp\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "# 1) Download your model\n",
        "model_path = hf_hub_download(\n",
        "    repo_id=\"TheBloke/Llama-2-7B-Chat-GGUF\",\n",
        "    filename=\"llama-2-7b-chat.Q4_0.gguf\",\n",
        "    local_dir=\"./models\",\n",
        "    local_dir_use_symlinks=False\n",
        ")\n",
        "\n",
        "# 2) Create LlamaCpp instance\n",
        "llm = LlamaCpp(\n",
        "    model_path=model_path,\n",
        "    temperature=0.7,\n",
        "    max_tokens=256,\n",
        "    top_p=0.95,\n",
        "    repeat_penalty=1.15,\n",
        "    n_ctx=1024,\n",
        "    n_gpu_layers=40,\n",
        "    n_batch=512,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# 3) Create your prompt + LLMChain\n",
        "bot_prompt = PromptTemplate(\n",
        "    input_variables=[\"user_message\"],\n",
        "    template=(\n",
        "        \"You are a helpful airline booking assistant for Fictitious Air.\\n\"\n",
        "        \"User: {user_message}\\n\"\n",
        "        \"Assistant:\"\n",
        "    ),\n",
        ")\n",
        "response_chain = LLMChain(llm=llm, prompt=bot_prompt)\n",
        "\n",
        "# 4) Then proceed with the rest of your code\n",
        "# (dummy_flights_data, process_user_input, etc.)\n",
        "# ...\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R62ofbunlXwc",
        "outputId": "00a00bcd-00d0-4c3c-bc85-b7f2510b19f5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:834: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\n",
            "For more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n",
            "  warnings.warn(\n",
            "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from models/llama-2-7b-chat.Q4_0.gguf (version GGUF V2)\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
            "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
            "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
            "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
            "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
            "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q4_0:  225 tensors\n",
            "llama_model_loader: - type q6_K:    1 tensors\n",
            "print_info: file format = GGUF V2\n",
            "print_info: file type   = Q4_0\n",
            "print_info: file size   = 3.56 GiB (4.54 BPW) \n",
            "init_tokenizer: initializing tokenizer for type 1\n",
            "load: control token:      2 '</s>' is not marked as EOG\n",
            "load: control token:      1 '<s>' is not marked as EOG\n",
            "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
            "load: special tokens cache size = 3\n",
            "load: token to piece cache size = 0.1684 MB\n",
            "print_info: arch             = llama\n",
            "print_info: vocab_only       = 0\n",
            "print_info: n_ctx_train      = 4096\n",
            "print_info: n_embd           = 4096\n",
            "print_info: n_layer          = 32\n",
            "print_info: n_head           = 32\n",
            "print_info: n_head_kv        = 32\n",
            "print_info: n_rot            = 128\n",
            "print_info: n_swa            = 0\n",
            "print_info: n_embd_head_k    = 128\n",
            "print_info: n_embd_head_v    = 128\n",
            "print_info: n_gqa            = 1\n",
            "print_info: n_embd_k_gqa     = 4096\n",
            "print_info: n_embd_v_gqa     = 4096\n",
            "print_info: f_norm_eps       = 0.0e+00\n",
            "print_info: f_norm_rms_eps   = 1.0e-06\n",
            "print_info: f_clamp_kqv      = 0.0e+00\n",
            "print_info: f_max_alibi_bias = 0.0e+00\n",
            "print_info: f_logit_scale    = 0.0e+00\n",
            "print_info: f_attn_scale     = 0.0e+00\n",
            "print_info: n_ff             = 11008\n",
            "print_info: n_expert         = 0\n",
            "print_info: n_expert_used    = 0\n",
            "print_info: causal attn      = 1\n",
            "print_info: pooling type     = 0\n",
            "print_info: rope type        = 0\n",
            "print_info: rope scaling     = linear\n",
            "print_info: freq_base_train  = 10000.0\n",
            "print_info: freq_scale_train = 1\n",
            "print_info: n_ctx_orig_yarn  = 4096\n",
            "print_info: rope_finetuned   = unknown\n",
            "print_info: ssm_d_conv       = 0\n",
            "print_info: ssm_d_inner      = 0\n",
            "print_info: ssm_d_state      = 0\n",
            "print_info: ssm_dt_rank      = 0\n",
            "print_info: ssm_dt_b_c_rms   = 0\n",
            "print_info: model type       = 7B\n",
            "print_info: model params     = 6.74 B\n",
            "print_info: general.name     = LLaMA v2\n",
            "print_info: vocab type       = SPM\n",
            "print_info: n_vocab          = 32000\n",
            "print_info: n_merges         = 0\n",
            "print_info: BOS token        = 1 '<s>'\n",
            "print_info: EOS token        = 2 '</s>'\n",
            "print_info: UNK token        = 0 '<unk>'\n",
            "print_info: LF token         = 13 '<0x0A>'\n",
            "print_info: EOG token        = 2 '</s>'\n",
            "print_info: max token length = 48\n",
            "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
            "load_tensors: layer   0 assigned to device CPU\n",
            "load_tensors: layer   1 assigned to device CPU\n",
            "load_tensors: layer   2 assigned to device CPU\n",
            "load_tensors: layer   3 assigned to device CPU\n",
            "load_tensors: layer   4 assigned to device CPU\n",
            "load_tensors: layer   5 assigned to device CPU\n",
            "load_tensors: layer   6 assigned to device CPU\n",
            "load_tensors: layer   7 assigned to device CPU\n",
            "load_tensors: layer   8 assigned to device CPU\n",
            "load_tensors: layer   9 assigned to device CPU\n",
            "load_tensors: layer  10 assigned to device CPU\n",
            "load_tensors: layer  11 assigned to device CPU\n",
            "load_tensors: layer  12 assigned to device CPU\n",
            "load_tensors: layer  13 assigned to device CPU\n",
            "load_tensors: layer  14 assigned to device CPU\n",
            "load_tensors: layer  15 assigned to device CPU\n",
            "load_tensors: layer  16 assigned to device CPU\n",
            "load_tensors: layer  17 assigned to device CPU\n",
            "load_tensors: layer  18 assigned to device CPU\n",
            "load_tensors: layer  19 assigned to device CPU\n",
            "load_tensors: layer  20 assigned to device CPU\n",
            "load_tensors: layer  21 assigned to device CPU\n",
            "load_tensors: layer  22 assigned to device CPU\n",
            "load_tensors: layer  23 assigned to device CPU\n",
            "load_tensors: layer  24 assigned to device CPU\n",
            "load_tensors: layer  25 assigned to device CPU\n",
            "load_tensors: layer  26 assigned to device CPU\n",
            "load_tensors: layer  27 assigned to device CPU\n",
            "load_tensors: layer  28 assigned to device CPU\n",
            "load_tensors: layer  29 assigned to device CPU\n",
            "load_tensors: layer  30 assigned to device CPU\n",
            "load_tensors: layer  31 assigned to device CPU\n",
            "load_tensors: layer  32 assigned to device CPU\n",
            "load_tensors: tensor 'token_embd.weight' (q4_0) (and 66 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
            "load_tensors:  CPU_AARCH64 model buffer size =  3474.00 MiB\n",
            "load_tensors:   CPU_Mapped model buffer size =  3647.87 MiB\n",
            "repack: repack tensor blk.0.attn_q.weight with q4_0_8x8\n",
            "repack: repack tensor blk.0.attn_k.weight with q4_0_8x8\n",
            "repack: repack tensor blk.0.attn_v.weight with q4_0_8x8\n",
            "repack: repack tensor blk.0.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.0.ffn_gate.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.0.ffn_down.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.0.ffn_up.weight with q4_0_8x8\n",
            "repack: repack tensor blk.1.attn_q.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.1.attn_k.weight with q4_0_8x8\n",
            "repack: repack tensor blk.1.attn_v.weight with q4_0_8x8\n",
            "repack: repack tensor blk.1.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.1.ffn_gate.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.1.ffn_down.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.1.ffn_up.weight with q4_0_8x8\n",
            "repack: repack tensor blk.2.attn_q.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.2.attn_k.weight with q4_0_8x8\n",
            "repack: repack tensor blk.2.attn_v.weight with q4_0_8x8\n",
            "repack: repack tensor blk.2.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.2.ffn_gate.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.2.ffn_down.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.2.ffn_up.weight with q4_0_8x8\n",
            "repack: repack tensor blk.3.attn_q.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.3.attn_k.weight with q4_0_8x8\n",
            "repack: repack tensor blk.3.attn_v.weight with q4_0_8x8\n",
            "repack: repack tensor blk.3.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.3.ffn_gate.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.3.ffn_down.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.3.ffn_up.weight with q4_0_8x8\n",
            "repack: repack tensor blk.4.attn_q.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.4.attn_k.weight with q4_0_8x8\n",
            "repack: repack tensor blk.4.attn_v.weight with q4_0_8x8\n",
            "repack: repack tensor blk.4.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.4.ffn_gate.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.4.ffn_down.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.4.ffn_up.weight with q4_0_8x8\n",
            "repack: repack tensor blk.5.attn_q.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.5.attn_k.weight with q4_0_8x8\n",
            "repack: repack tensor blk.5.attn_v.weight with q4_0_8x8\n",
            "repack: repack tensor blk.5.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.5.ffn_gate.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.5.ffn_down.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.5.ffn_up.weight with q4_0_8x8\n",
            "repack: repack tensor blk.6.attn_q.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.6.attn_k.weight with q4_0_8x8\n",
            "repack: repack tensor blk.6.attn_v.weight with q4_0_8x8\n",
            "repack: repack tensor blk.6.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.6.ffn_gate.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.6.ffn_down.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.6.ffn_up.weight with q4_0_8x8\n",
            "repack: repack tensor blk.7.attn_q.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.7.attn_k.weight with q4_0_8x8\n",
            "repack: repack tensor blk.7.attn_v.weight with q4_0_8x8\n",
            "repack: repack tensor blk.7.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.7.ffn_gate.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.7.ffn_down.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.7.ffn_up.weight with q4_0_8x8\n",
            "repack: repack tensor blk.8.attn_q.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.8.attn_k.weight with q4_0_8x8\n",
            "repack: repack tensor blk.8.attn_v.weight with q4_0_8x8\n",
            "repack: repack tensor blk.8.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.8.ffn_gate.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.8.ffn_down.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.8.ffn_up.weight with q4_0_8x8\n",
            "repack: repack tensor blk.9.attn_q.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.9.attn_k.weight with q4_0_8x8\n",
            "repack: repack tensor blk.9.attn_v.weight with q4_0_8x8\n",
            "repack: repack tensor blk.9.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.9.ffn_gate.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.9.ffn_down.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.9.ffn_up.weight with q4_0_8x8\n",
            "repack: repack tensor blk.10.attn_q.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.10.attn_k.weight with q4_0_8x8\n",
            "repack: repack tensor blk.10.attn_v.weight with q4_0_8x8\n",
            "repack: repack tensor blk.10.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.10.ffn_gate.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.10.ffn_down.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.10.ffn_up.weight with q4_0_8x8\n",
            "repack: repack tensor blk.11.attn_q.weight with q4_0_8x8\n",
            "repack: repack tensor blk.11.attn_k.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.11.attn_v.weight with q4_0_8x8\n",
            "repack: repack tensor blk.11.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.11.ffn_gate.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.11.ffn_down.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.11.ffn_up.weight with q4_0_8x8\n",
            "repack: repack tensor blk.12.attn_q.weight with q4_0_8x8\n",
            "repack: repack tensor blk.12.attn_k.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.12.attn_v.weight with q4_0_8x8\n",
            "repack: repack tensor blk.12.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.12.ffn_gate.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.12.ffn_down.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.12.ffn_up.weight with q4_0_8x8\n",
            "repack: repack tensor blk.13.attn_q.weight with q4_0_8x8\n",
            "repack: repack tensor blk.13.attn_k.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.13.attn_v.weight with q4_0_8x8\n",
            "repack: repack tensor blk.13.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.13.ffn_gate.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.13.ffn_down.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.13.ffn_up.weight with q4_0_8x8\n",
            "repack: repack tensor blk.14.attn_q.weight with q4_0_8x8\n",
            "repack: repack tensor blk.14.attn_k.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.14.attn_v.weight with q4_0_8x8\n",
            "repack: repack tensor blk.14.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.14.ffn_gate.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.14.ffn_down.weight with q4_0_8x8\n",
            "repack: repack tensor blk.14.ffn_up.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.15.attn_q.weight with q4_0_8x8\n",
            "repack: repack tensor blk.15.attn_k.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.15.attn_v.weight with q4_0_8x8\n",
            "repack: repack tensor blk.15.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.15.ffn_gate.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.15.ffn_down.weight with q4_0_8x8\n",
            "repack: repack tensor blk.15.ffn_up.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.16.attn_q.weight with q4_0_8x8\n",
            "repack: repack tensor blk.16.attn_k.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.16.attn_v.weight with q4_0_8x8\n",
            "repack: repack tensor blk.16.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.16.ffn_gate.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.16.ffn_down.weight with q4_0_8x8\n",
            "repack: repack tensor blk.16.ffn_up.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.17.attn_q.weight with q4_0_8x8\n",
            "repack: repack tensor blk.17.attn_k.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.17.attn_v.weight with q4_0_8x8\n",
            "repack: repack tensor blk.17.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.17.ffn_gate.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.17.ffn_down.weight with q4_0_8x8\n",
            "repack: repack tensor blk.17.ffn_up.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.18.attn_q.weight with q4_0_8x8\n",
            "repack: repack tensor blk.18.attn_k.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.18.attn_v.weight with q4_0_8x8\n",
            "repack: repack tensor blk.18.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.18.ffn_gate.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.18.ffn_down.weight with q4_0_8x8\n",
            "repack: repack tensor blk.18.ffn_up.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.19.attn_q.weight with q4_0_8x8\n",
            "repack: repack tensor blk.19.attn_k.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.19.attn_v.weight with q4_0_8x8\n",
            "repack: repack tensor blk.19.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.19.ffn_gate.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.19.ffn_down.weight with q4_0_8x8\n",
            "repack: repack tensor blk.19.ffn_up.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.20.attn_q.weight with q4_0_8x8\n",
            "repack: repack tensor blk.20.attn_k.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.20.attn_v.weight with q4_0_8x8\n",
            "repack: repack tensor blk.20.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.20.ffn_gate.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.20.ffn_down.weight with q4_0_8x8\n",
            "repack: repack tensor blk.20.ffn_up.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.21.attn_q.weight with q4_0_8x8\n",
            "repack: repack tensor blk.21.attn_k.weight with q4_0_8x8\n",
            "repack: repack tensor blk.21.attn_v.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.21.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.21.ffn_gate.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.21.ffn_down.weight with q4_0_8x8\n",
            "repack: repack tensor blk.21.ffn_up.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.22.attn_q.weight with q4_0_8x8\n",
            "repack: repack tensor blk.22.attn_k.weight with q4_0_8x8\n",
            "repack: repack tensor blk.22.attn_v.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.22.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.22.ffn_gate.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.22.ffn_down.weight with q4_0_8x8\n",
            "repack: repack tensor blk.22.ffn_up.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.23.attn_q.weight with q4_0_8x8\n",
            "repack: repack tensor blk.23.attn_k.weight with q4_0_8x8\n",
            "repack: repack tensor blk.23.attn_v.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.23.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.23.ffn_gate.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.23.ffn_down.weight with q4_0_8x8\n",
            "repack: repack tensor blk.23.ffn_up.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.24.attn_q.weight with q4_0_8x8\n",
            "repack: repack tensor blk.24.attn_k.weight with q4_0_8x8\n",
            "repack: repack tensor blk.24.attn_v.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.24.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.24.ffn_gate.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.24.ffn_down.weight with q4_0_8x8\n",
            "repack: repack tensor blk.24.ffn_up.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.25.attn_q.weight with q4_0_8x8\n",
            "repack: repack tensor blk.25.attn_k.weight with q4_0_8x8\n",
            "repack: repack tensor blk.25.attn_v.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.25.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.25.ffn_gate.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.25.ffn_down.weight with q4_0_8x8\n",
            "repack: repack tensor blk.25.ffn_up.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.26.attn_q.weight with q4_0_8x8\n",
            "repack: repack tensor blk.26.attn_k.weight with q4_0_8x8\n",
            "repack: repack tensor blk.26.attn_v.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.26.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.26.ffn_gate.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.26.ffn_down.weight with q4_0_8x8\n",
            "repack: repack tensor blk.26.ffn_up.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.27.attn_q.weight with q4_0_8x8\n",
            "repack: repack tensor blk.27.attn_k.weight with q4_0_8x8\n",
            "repack: repack tensor blk.27.attn_v.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.27.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.27.ffn_gate.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.27.ffn_down.weight with q4_0_8x8\n",
            "repack: repack tensor blk.27.ffn_up.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.28.attn_q.weight with q4_0_8x8\n",
            "repack: repack tensor blk.28.attn_k.weight with q4_0_8x8\n",
            "repack: repack tensor blk.28.attn_v.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.28.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.28.ffn_gate.weight with q4_0_8x8\n",
            "repack: repack tensor blk.28.ffn_down.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.28.ffn_up.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.29.attn_q.weight with q4_0_8x8\n",
            "repack: repack tensor blk.29.attn_k.weight with q4_0_8x8\n",
            "repack: repack tensor blk.29.attn_v.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.29.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.29.ffn_gate.weight with q4_0_8x8\n",
            "repack: repack tensor blk.29.ffn_down.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.29.ffn_up.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.30.attn_q.weight with q4_0_8x8\n",
            "repack: repack tensor blk.30.attn_k.weight with q4_0_8x8\n",
            "repack: repack tensor blk.30.attn_v.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.30.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.30.ffn_gate.weight with q4_0_8x8\n",
            "repack: repack tensor blk.30.ffn_down.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.30.ffn_up.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.31.attn_q.weight with q4_0_8x8\n",
            "repack: repack tensor blk.31.attn_k.weight with q4_0_8x8\n",
            "repack: repack tensor blk.31.attn_v.weight with q4_0_8x8\n",
            "repack: repack tensor blk.31.attn_output.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.31.ffn_gate.weight with q4_0_8x8\n",
            "repack: repack tensor blk.31.ffn_down.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.31.ffn_up.weight with q4_0_8x8\n",
            "....\n",
            "llama_init_from_model: n_seq_max     = 1\n",
            "llama_init_from_model: n_ctx         = 1024\n",
            "llama_init_from_model: n_ctx_per_seq = 1024\n",
            "llama_init_from_model: n_batch       = 512\n",
            "llama_init_from_model: n_ubatch      = 512\n",
            "llama_init_from_model: flash_attn    = 0\n",
            "llama_init_from_model: freq_base     = 10000.0\n",
            "llama_init_from_model: freq_scale    = 1\n",
            "llama_init_from_model: n_ctx_per_seq (1024) < n_ctx_train (4096) -- the full capacity of the model will not be utilized\n",
            "llama_kv_cache_init: kv_size = 1024, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1\n",
            "llama_kv_cache_init: layer 0: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
            "llama_kv_cache_init: layer 1: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
            "llama_kv_cache_init: layer 2: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
            "llama_kv_cache_init: layer 3: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
            "llama_kv_cache_init: layer 4: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
            "llama_kv_cache_init: layer 5: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
            "llama_kv_cache_init: layer 6: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
            "llama_kv_cache_init: layer 7: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
            "llama_kv_cache_init: layer 8: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
            "llama_kv_cache_init: layer 9: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
            "llama_kv_cache_init: layer 10: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
            "llama_kv_cache_init: layer 11: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
            "llama_kv_cache_init: layer 12: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
            "llama_kv_cache_init: layer 13: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
            "llama_kv_cache_init: layer 14: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
            "llama_kv_cache_init: layer 15: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
            "llama_kv_cache_init: layer 16: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
            "llama_kv_cache_init: layer 17: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
            "llama_kv_cache_init: layer 18: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
            "llama_kv_cache_init: layer 19: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
            "llama_kv_cache_init: layer 20: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
            "llama_kv_cache_init: layer 21: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
            "llama_kv_cache_init: layer 22: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
            "llama_kv_cache_init: layer 23: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
            "llama_kv_cache_init: layer 24: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
            "llama_kv_cache_init: layer 25: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
            "llama_kv_cache_init: layer 26: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
            "llama_kv_cache_init: layer 27: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
            "llama_kv_cache_init: layer 28: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
            "llama_kv_cache_init: layer 29: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
            "llama_kv_cache_init: layer 30: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
            "llama_kv_cache_init: layer 31: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
            "llama_kv_cache_init:        CPU KV buffer size =   512.00 MiB\n",
            "llama_init_from_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
            "llama_init_from_model:        CPU  output buffer size =     0.12 MiB\n",
            "llama_init_from_model:        CPU compute buffer size =    98.01 MiB\n",
            "llama_init_from_model: graph nodes  = 1030\n",
            "llama_init_from_model: graph splits = 1\n",
            "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
            "Model metadata: {'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'general.name': 'LLaMA v2', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '11008', 'llama.attention.layer_norm_rms_epsilon': '0.000001', 'llama.rope.dimension_count': '128', 'llama.attention.head_count': '32', 'tokenizer.ggml.bos_token_id': '1', 'llama.block_count': '32', 'llama.attention.head_count_kv': '32', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '2'}\n",
            "Using fallback chat format: llama-2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import re\n",
        "from transformers import pipeline\n",
        "\n",
        "###############################################################################\n",
        "# 1) Dummy Flights Data\n",
        "###############################################################################\n",
        "dummy_flights_data = {\n",
        "    \"flights\": [\n",
        "        {\n",
        "            \"flight_number\": \"FA101\",\n",
        "            \"origin\": \"NYC\",\n",
        "            \"destination\": \"LAX\",\n",
        "            \"departure_date\": \"2025-06-01\",\n",
        "            \"departure_time\": \"08:00\",\n",
        "            \"arrival_time\": \"11:00\",\n",
        "            \"price\": 320,\n",
        "            \"currency\": \"USD\",\n",
        "            \"seat_availability\": 60\n",
        "        },\n",
        "        {\n",
        "            \"flight_number\": \"FA102\",\n",
        "            \"origin\": \"NYC\",\n",
        "            \"destination\": \"LAX\",\n",
        "            \"departure_date\": \"2025-06-02\",\n",
        "            \"departure_time\": \"09:30\",\n",
        "            \"arrival_time\": \"12:30\",\n",
        "            \"price\": 300,\n",
        "            \"currency\": \"USD\",\n",
        "            \"seat_availability\": 45\n",
        "        },\n",
        "        {\n",
        "            \"flight_number\": \"FA103\",\n",
        "            \"origin\": \"NYC\",\n",
        "            \"destination\": \"LAX\",\n",
        "            \"departure_date\": \"2025-06-03\",\n",
        "            \"departure_time\": \"14:00\",\n",
        "            \"arrival_time\": \"17:00\",\n",
        "            \"price\": 340,\n",
        "            \"currency\": \"USD\",\n",
        "            \"seat_availability\": 80\n",
        "        },\n",
        "        {\n",
        "            \"flight_number\": \"FA104\",\n",
        "            \"origin\": \"LAX\",\n",
        "            \"destination\": \"NYC\",\n",
        "            \"departure_date\": \"2025-06-01\",\n",
        "            \"departure_time\": \"07:00\",\n",
        "            \"arrival_time\": \"15:00\",\n",
        "            \"price\": 350,\n",
        "            \"currency\": \"USD\",\n",
        "            \"seat_availability\": 55\n",
        "        },\n",
        "        {\n",
        "            \"flight_number\": \"FA105\",\n",
        "            \"origin\": \"LAX\",\n",
        "            \"destination\": \"NYC\",\n",
        "            \"departure_date\": \"2025-06-02\",\n",
        "            \"departure_time\": \"12:00\",\n",
        "            \"arrival_time\": \"20:00\",\n",
        "            \"price\": 330,\n",
        "            \"currency\": \"USD\",\n",
        "            \"seat_availability\": 72\n",
        "        },\n",
        "        {\n",
        "            \"flight_number\": \"FA106\",\n",
        "            \"origin\": \"LAX\",\n",
        "            \"destination\": \"NYC\",\n",
        "            \"departure_date\": \"2025-06-03\",\n",
        "            \"departure_time\": \"16:00\",\n",
        "            \"arrival_time\": \"00:30\",\n",
        "            \"price\": 320,\n",
        "            \"currency\": \"USD\",\n",
        "            \"seat_availability\": 40\n",
        "        }\n",
        "    ]\n",
        "}\n"
      ],
      "metadata": {
        "id": "kta1LUHUqB1q"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###############################################################################\n",
        "# 2) Helper Functions: route extraction, name extraction\n",
        "###############################################################################\n",
        "def extract_route_from_text(text):\n",
        "    text = text.upper().strip(\".,!?;'\\\" \\t\\n\\r\")\n",
        "    pattern = r\"FROM\\s+([A-Z]{3})\\s+TO\\s+([A-Z]{3})\"\n",
        "    match = re.search(pattern, text)\n",
        "    if match:\n",
        "        return match.group(1), match.group(2)\n",
        "    return None, None\n",
        "\n",
        "def extract_name(text):\n",
        "    text = text.strip()\n",
        "    match = re.search(r\"(?:my name is|i am|i'm)\\s+([A-Z][a-zA-Z]+)\", text, re.IGNORECASE)\n",
        "    if match:\n",
        "        return match.group(1).strip()\n",
        "    words = text.split()\n",
        "    if len(words) == 1:\n",
        "        return words[0]\n",
        "    return None"
      ],
      "metadata": {
        "id": "JKhVHbkoqEP7"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###############################################################################\n",
        "# 3) Zero-shot Classifier (No LLMChain)\n",
        "###############################################################################\n",
        "classifier = pipeline(\"zero-shot-classification\", model=\"valhalla/distilbart-mnli-12-3\")\n",
        "\n",
        "def classify_intent(text):\n",
        "    text_lower = text.lower().strip(\".,!?;'\\\" \\t\\n\\r\")\n",
        "\n",
        "    if (\"from\" in text_lower and \"to\" in text_lower) or (\"->\" in text_lower):\n",
        "        return \"flight_info\"\n",
        "    if \"cancel\" in text_lower and (\"flight\" in text_lower or \"booking\" in text_lower):\n",
        "        return \"cancel_flight\"\n",
        "    if \"reschedul\" in text_lower and \"flight\" in text_lower:\n",
        "        return \"reschedule_flight\"\n",
        "    if \"book\" in text_lower and \"flight\" in text_lower:\n",
        "        return \"book_flight\"\n",
        "    if ((\"which\" in text_lower or \"list\" in text_lower or \"show\" in text_lower)\n",
        "        and (\"booked\" in text_lower or \"bookings\" in text_lower or \"booking\" in text_lower)):\n",
        "        return \"list_bookings\"\n",
        "    if (\"no thanks\" in text_lower or \"no thank you\" in text_lower or\n",
        "        \"bye\" in text_lower or \"i'm done\" in text_lower):\n",
        "        return \"exit\"\n",
        "\n",
        "    labels = [\"greet\", \"book_flight\", \"flight_info\", \"user_info\", \"exit\", \"fallback\"]\n",
        "    try:\n",
        "        result = classifier(text, labels)\n",
        "        return result[\"labels\"][0]\n",
        "    except:\n",
        "        return \"fallback\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EwM8yj_UqHtz",
        "outputId": "f2410de0-2cc8-416f-954c-fa43652d4999"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###############################################################################\n",
        "# 4) Global State\n",
        "###############################################################################\n",
        "conversation_ended = False\n",
        "bookings = []\n",
        "user_info = {\n",
        "    \"name\": None,                # main contact name\n",
        "    \"email\": None,               # main contact email\n",
        "    \"flight_choice\": None,\n",
        "    \"available_flight_options\": [],\n",
        "    \"passenger_count\": 1,\n",
        "    \"passenger_names\": [],\n",
        "    \"baggage_count\": 0,\n",
        "    \"final_cost\": 0.0\n",
        "}\n",
        "next_action = None"
      ],
      "metadata": {
        "id": "sz1w3nINqKgc"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###############################################################################\n",
        "# 5) Payment & Confirmation Helpers\n",
        "###############################################################################\n",
        "import re\n",
        "\n",
        "def handle_payment_step(user_message):\n",
        "    global next_action, bookings, user_info\n",
        "    card = user_message.strip().replace(\" \", \"\")\n",
        "    if re.match(r\"^\\d{16}$\", card):\n",
        "        chosen_flight = user_info[\"flight_choice\"]\n",
        "        price_charged = user_info[\"final_cost\"]\n",
        "        booking_record = {\n",
        "            \"flight_number\": chosen_flight[\"flight_number\"],\n",
        "            \"user_name\": user_info[\"name\"],\n",
        "            \"email\": user_info[\"email\"],\n",
        "            \"price\": price_charged,\n",
        "            \"date\": chosen_flight[\"departure_date\"],\n",
        "            \"origin\": chosen_flight[\"origin\"],\n",
        "            \"destination\": chosen_flight[\"destination\"],\n",
        "            \"passenger_count\": user_info[\"passenger_count\"],\n",
        "            \"passenger_names\": user_info[\"passenger_names\"],\n",
        "            \"baggage_count\": user_info[\"baggage_count\"]\n",
        "        }\n",
        "        bookings.append(booking_record)\n",
        "\n",
        "        next_action = None\n",
        "        return (\n",
        "            f\"Payment successful! We have charged ${price_charged} to your card.\\n\"\n",
        "            f\"Booking confirmed for {user_info['name']}!\\n\"\n",
        "            f\"Flight: {chosen_flight['flight_number']} on {chosen_flight['departure_date']} \"\n",
        "            f\"({chosen_flight['origin']}->{chosen_flight['destination']})\\n\"\n",
        "            f\"Total passengers: {user_info['passenger_count']} (names: {', '.join(user_info['passenger_names'])})\\n\"\n",
        "            f\"Extra checked bags: {user_info['baggage_count']}\\n\"\n",
        "            f\"Email: {user_info['email']}\\n\"\n",
        "            \"You're all set! Anything else I can help you with?\"\n",
        "        )\n",
        "    else:\n",
        "        return \"That doesn't look like a valid 16-digit card number. Please try again.\"\n",
        "\n",
        "def summarize_booking():\n",
        "    chosen = user_info[\"flight_choice\"]\n",
        "    pcount = user_info[\"passenger_count\"]\n",
        "    names_str = \", \".join(user_info[\"passenger_names\"])\n",
        "    flight_info = (\n",
        "        f\"{chosen['flight_number']} on {chosen['departure_date']} \"\n",
        "        f\"({chosen['origin']}->{chosen['destination']})\"\n",
        "    )\n",
        "    extra_bags = user_info[\"baggage_count\"]\n",
        "    final_cost = user_info[\"final_cost\"]\n",
        "    return (\n",
        "        \"Please review your booking details:\\n\"\n",
        "        f\"Flight: {flight_info}\\n\"\n",
        "        f\"Main contact: {user_info['name']} (email: {user_info['email']})\\n\"\n",
        "        f\"Passengers: {pcount}\\n\"\n",
        "        f\"Passenger names: {names_str}\\n\"\n",
        "        f\"Extra checked bags: {extra_bags}\\n\"\n",
        "        f\"Total cost: ${final_cost}\\n\"\n",
        "        \"Do you confirm this booking? (yes/no)\"\n",
        "    )\n"
      ],
      "metadata": {
        "id": "23lS9cTDqN8f"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "8hQso24Qh7V7"
      },
      "outputs": [],
      "source": [
        "###############################################################################\n",
        "# 6) Main Bot Logic (Now we ask for name+email after flight is chosen)\n",
        "###############################################################################\n",
        "def process_user_input(user_message):\n",
        "    global conversation_ended, next_action, user_info, bookings\n",
        "\n",
        "    if conversation_ended:\n",
        "        return \"Conversation closed. Please refresh to start a new one.\"\n",
        "\n",
        "    user_message = user_message.strip(\".,!?;'\\\" \\t\\n\\r\")\n",
        "\n",
        "    # A) Payment step\n",
        "    if next_action == \"get_payment\":\n",
        "        return handle_payment_step(user_message)\n",
        "\n",
        "    # B) Final confirmation\n",
        "    if next_action == \"confirm_booking\":\n",
        "        msg_lower = user_message.lower()\n",
        "        if \"yes\" in msg_lower:\n",
        "            next_action = \"get_payment\"\n",
        "            return \"Great! Please provide your 16-digit credit card number to complete payment.\"\n",
        "        else:\n",
        "            # user declined => start route selection again\n",
        "            next_action = \"get_flight_choice\"\n",
        "            return \"Okay, let's start over. Which route would you like to book? (e.g. 'NYC -> LAX')\"\n",
        "\n",
        "    # C) If we are collecting main contact name\n",
        "    if next_action == \"get_main_contact\":\n",
        "        # parse name\n",
        "        parsed_name = extract_name(user_message)\n",
        "        if parsed_name:\n",
        "            user_info[\"name\"] = parsed_name\n",
        "        else:\n",
        "            user_info[\"name\"] = user_message.strip()\n",
        "        next_action = \"get_email\"\n",
        "        return f\"Thanks, {user_info['name']}! Please provide your email.\"\n",
        "\n",
        "    # D) If we are collecting main contact email\n",
        "    if next_action == \"get_email\":\n",
        "        # just store whatever user typed\n",
        "        user_info[\"email\"] = user_message.strip()\n",
        "        # proceed to passenger_count\n",
        "        next_action = \"get_passenger_count\"\n",
        "        return (\n",
        "            \"Got it. Now, how many passengers in total (including you)?\"\n",
        "        )\n",
        "\n",
        "    # E) If we are collecting passenger count\n",
        "    if next_action == \"get_passenger_count\":\n",
        "        if user_message.isdigit():\n",
        "            count = int(user_message)\n",
        "            if count < 1:\n",
        "                count = 1\n",
        "            user_info[\"passenger_count\"] = count\n",
        "            user_info[\"passenger_names\"] = []\n",
        "            if count == 1:\n",
        "                # If there's only 1 passenger, that's the main contact\n",
        "                user_info[\"passenger_names\"].append(user_info[\"name\"])\n",
        "                next_action = \"get_baggage\"\n",
        "                return (\n",
        "                    f\"Got it. {count} passenger total.\\n\"\n",
        "                    \"Each passenger can bring one personal item + one carry-on for free.\\n\"\n",
        "                    \"Any additional checked bag costs $40. How many extra bags in total?\"\n",
        "                )\n",
        "            else:\n",
        "                # If multiple, we ask for each passenger name\n",
        "                next_action = \"get_passenger_names\"\n",
        "                return (\n",
        "                    f\"Please provide the names of the {count} passengers, one by one.\\n\"\n",
        "                    \"What is the first passenger's name?\"\n",
        "                )\n",
        "        else:\n",
        "            return \"Please enter a valid number of passengers.\"\n",
        "\n",
        "    # F) If collecting multiple passenger names\n",
        "    if next_action == \"get_passenger_names\":\n",
        "        parsed_name = extract_name(user_message)\n",
        "        if parsed_name:\n",
        "            user_info[\"passenger_names\"].append(parsed_name)\n",
        "        else:\n",
        "            user_info[\"passenger_names\"].append(user_message.strip())\n",
        "\n",
        "        if len(user_info[\"passenger_names\"]) < user_info[\"passenger_count\"]:\n",
        "            return \"Thanks. Next passenger's name?\"\n",
        "        else:\n",
        "            # done collecting passenger names\n",
        "            next_action = \"get_baggage\"\n",
        "            return (\n",
        "                f\"Great, got all {user_info['passenger_count']} names.\\n\"\n",
        "                \"Each passenger gets 1 personal item + 1 carry-on free.\\n\"\n",
        "                \"Any extra checked bag costs $40. How many total extra bags?\"\n",
        "            )\n",
        "\n",
        "    # G) If collecting baggage\n",
        "    if next_action == \"get_baggage\":\n",
        "        if user_message.isdigit():\n",
        "            extra_bags = int(user_message)\n",
        "            if extra_bags < 0:\n",
        "                extra_bags = 0\n",
        "            user_info[\"baggage_count\"] = extra_bags\n",
        "\n",
        "            chosen = user_info[\"flight_choice\"]\n",
        "            pcount = user_info[\"passenger_count\"]\n",
        "            base_price = chosen[\"price\"] * pcount\n",
        "            bag_cost = extra_bags * 40\n",
        "            total = base_price + bag_cost\n",
        "            user_info[\"final_cost\"] = total\n",
        "\n",
        "            next_action = \"confirm_booking\"\n",
        "            return summarize_booking()\n",
        "        else:\n",
        "            return \"Please enter a valid number for extra bags.\"\n",
        "\n",
        "    # H) If we are waiting for flight number (booking)\n",
        "    if next_action == \"get_flight_number\":\n",
        "        flight_number = user_message.upper()\n",
        "        chosen = next(\n",
        "            (f for f in user_info[\"available_flight_options\"]\n",
        "             if f[\"flight_number\"] == flight_number),\n",
        "            None\n",
        "        )\n",
        "        if chosen:\n",
        "            user_info[\"flight_choice\"] = chosen\n",
        "            # Now we ask for main contact name\n",
        "            next_action = \"get_main_contact\"\n",
        "            return (\n",
        "                f\"Great choice! You chose {flight_number}.\\n\"\n",
        "                \"May I have your name (main contact)?\"\n",
        "            )\n",
        "        else:\n",
        "            return \"I couldn't find that flight number. Please try again.\"\n",
        "\n",
        "    # I) If we are waiting for flight number (cancellation)\n",
        "    if next_action == \"get_cancellation_flight\":\n",
        "        flight_number = user_message.strip().upper()\n",
        "        booking_to_cancel = next(\n",
        "            (b for b in bookings if b[\"flight_number\"] == flight_number),\n",
        "            None\n",
        "        )\n",
        "        if booking_to_cancel:\n",
        "            bookings.remove(booking_to_cancel)\n",
        "            refund_amount = booking_to_cancel[\"price\"]\n",
        "            next_action = None\n",
        "            return (\n",
        "                f\"Your booking for flight {flight_number} has been canceled.\\n\"\n",
        "                f\"A refund of ${refund_amount} will be processed.\\n\"\n",
        "                \"Anything else I can do?\"\n",
        "            )\n",
        "        else:\n",
        "            # user might say \"which flights do I have booked?\" or something else\n",
        "            text_lower = user_message.lower()\n",
        "            if (\"which\" in text_lower or \"what\" in text_lower or \"list\" in text_lower):\n",
        "                user_intent_temp = classify_intent(text_lower)\n",
        "                if user_intent_temp == \"list_bookings\":\n",
        "                    if not bookings:\n",
        "                        return \"You have no bookings on record.\"\n",
        "                    else:\n",
        "                        listing = \"\\n\".join(\n",
        "                            f\"{b['flight_number']} on {b['date']} \"\n",
        "                            f\"({b['origin']}->{b['destination']})\"\n",
        "                            for b in bookings\n",
        "                        )\n",
        "                        return (\n",
        "                            \"Here are the flights you have booked:\\n\"\n",
        "                            + listing\n",
        "                            + \"\\n\\nWhich one would you like to cancel?\"\n",
        "                        )\n",
        "                else:\n",
        "                    return (\n",
        "                        \"I couldn't find that booking in your record. \"\n",
        "                        \"If you have multiple bookings, ask 'Which flights do I have booked?' \"\n",
        "                        \"or type the flight number you want to cancel.\"\n",
        "                    )\n",
        "            else:\n",
        "                return (\n",
        "                    \"I couldn't find that booking in your record. \"\n",
        "                    \"Please type the flight number you want to cancel, or ask me which you have booked.\"\n",
        "                )\n",
        "\n",
        "    # J) If we are waiting for route\n",
        "    if next_action == \"get_flight_choice\":\n",
        "        origin, dest = extract_route_from_text(user_message)\n",
        "        if not origin or not dest:\n",
        "            parts = user_message.upper().split(\"->\")\n",
        "            if len(parts) == 2:\n",
        "                origin, dest = parts[0].strip(), parts[1].strip()\n",
        "\n",
        "        if origin and dest:\n",
        "            flights = [\n",
        "                f for f in dummy_flights_data[\"flights\"]\n",
        "                if f[\"origin\"] == origin and f[\"destination\"] == dest\n",
        "            ]\n",
        "            if flights:\n",
        "                user_info[\"available_flight_options\"] = flights\n",
        "                flights_list = \"\\n\".join([\n",
        "                    f\"{f['flight_number']} on {f['departure_date']} \"\n",
        "                    f\"at {f['departure_time']} - ${f['price']}\"\n",
        "                    for f in flights\n",
        "                ])\n",
        "                next_action = \"get_flight_number\"\n",
        "                return (\n",
        "                    \"Here are your flight options:\\n\"\n",
        "                    + flights_list\n",
        "                    + \"\\n\\nWhich flight number would you like to book?\"\n",
        "                )\n",
        "            else:\n",
        "                next_action = None\n",
        "                return \"No flights found for that route.\"\n",
        "        else:\n",
        "            return \"Please specify a route, e.g. 'I want to fly from NYC to LAX.'\"\n",
        "\n",
        "    # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "    # K) Otherwise do classification\n",
        "    # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "    intent = classify_intent(user_message)\n",
        "\n",
        "    if intent == \"exit\":\n",
        "        conversation_ended = True\n",
        "        return \"Thanks for flying with Fictitious Air. Goodbye!\"\n",
        "\n",
        "    if intent == \"book_flight\":\n",
        "        next_action = \"get_flight_choice\"\n",
        "        return \"Sure! Which route would you like to book? (e.g. 'from NYC to LAX')\"\n",
        "\n",
        "    if intent == \"flight_info\":\n",
        "        next_action = \"get_flight_choice\"\n",
        "        return \"Which route do you want info for? e.g. 'NYC -> LAX' or 'from NYC to LAX.'\"\n",
        "\n",
        "    if intent == \"cancel_flight\":\n",
        "        next_action = \"get_cancellation_flight\"\n",
        "        return \"Sure, which flight number would you like to cancel?\"\n",
        "\n",
        "    if intent == \"reschedule_flight\":\n",
        "        next_action = \"get_reschedule_flight\"\n",
        "        return \"Sure, which flight number do you want to reschedule?\"\n",
        "\n",
        "    if intent == \"list_bookings\":\n",
        "        if not bookings:\n",
        "            return \"You have no bookings on record.\"\n",
        "        else:\n",
        "            listing = \"\\n\".join(\n",
        "                f\"{b['flight_number']} on {b['date']} \"\n",
        "                f\"({b['origin']}->{b['destination']}), \"\n",
        "                f\"{b['passenger_count']} passengers, \"\n",
        "                f\"${b['price']} total\"\n",
        "                for b in bookings\n",
        "            )\n",
        "            return \"Here are your current bookings:\\n\" + listing\n",
        "\n",
        "    if intent == \"fallback\":\n",
        "        return \"Sorry, I didn’t quite catch that. Can you rephrase?\"\n",
        "\n",
        "    # default fallback\n",
        "    return (\n",
        "        \"No LLM is available to handle that request. \"\n",
        "        \"Please try something else or type 'help' for assistance.\"\n",
        "    )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###############################################################################\n",
        "# 7) Gradio UI\n",
        "###############################################################################\n",
        "def chat_with_bot(message, history):\n",
        "    reply = process_user_input(message)\n",
        "    history.append((message, reply))\n",
        "    return history, \"\"\n",
        "\n",
        "initial_bot_message = (\n",
        "    \"Hello! How can I assist you today? \"\n",
        "    \"You can book, cancel, or reschedule flights, list your bookings, or update info.\"\n",
        ")\n",
        "\n",
        "import gradio as gr\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## ✈️ Fictitious Air Chatbot (Fixed: Always ask for name & email)\")\n",
        "    chatbot = gr.Chatbot(value=[[None, initial_bot_message]])\n",
        "    user_input = gr.Textbox(placeholder=\"Ask me about flights...\", show_label=False)\n",
        "    clear_btn = gr.Button(\"Clear\")\n",
        "\n",
        "    user_input.submit(chat_with_bot, [user_input, chatbot], [chatbot, user_input])\n",
        "    clear_btn.click(lambda: ([[None, initial_bot_message]], \"\"), None, [chatbot, user_input])\n",
        "\n",
        "demo.launch(inline=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 689
        },
        "id": "YnzwVsvyqRYC",
        "outputId": "eb8d197c-ec47-417d-f985-0394392b73d0"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-17-9e9ca8d6a3dd>:18: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot = gr.Chatbot(value=[[None, initial_bot_message]])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://d9164bc56a96cf163a.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://d9164bc56a96cf163a.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dphkZUaqiC1B"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}